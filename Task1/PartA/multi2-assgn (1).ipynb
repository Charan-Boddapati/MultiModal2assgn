{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-07T17:51:14.064912Z",
     "iopub.status.busy": "2025-03-07T17:51:14.064510Z",
     "iopub.status.idle": "2025-03-07T17:51:15.326498Z",
     "shell.execute_reply": "2025-03-07T17:51:15.324968Z",
     "shell.execute_reply.started": "2025-03-07T17:51:14.064882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import csv\n",
    "from google_images_search import GoogleImagesSearch\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T17:50:57.151577Z",
     "iopub.status.busy": "2025-03-07T17:50:57.151232Z",
     "iopub.status.idle": "2025-03-07T17:51:10.220468Z",
     "shell.execute_reply": "2025-03-07T17:51:10.218780Z",
     "shell.execute_reply.started": "2025-03-07T17:50:57.151550Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-images-search\n",
      "  Downloading Google_Images_Search-1.4.7-py2.py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: colorama~=0.4 in /usr/local/lib/python3.10/dist-packages (from google-images-search) (0.4.6)\n",
      "Collecting pyfiglet~=0.8 (from google-images-search)\n",
      "  Downloading pyfiglet-0.8.post1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor~=1.1 (from google-images-search)\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: click<=8.2,>=7.0 in /usr/local/lib/python3.10/dist-packages (from google-images-search) (8.1.7)\n",
      "Requirement already satisfied: six~=1.12 in /usr/local/lib/python3.10/dist-packages (from google-images-search) (1.17.0)\n",
      "Requirement already satisfied: requests~=2.21 in /usr/local/lib/python3.10/dist-packages (from google-images-search) (2.32.3)\n",
      "Requirement already satisfied: Pillow>=8.1.1 in /usr/local/lib/python3.10/dist-packages (from google-images-search) (11.0.0)\n",
      "Collecting python-resize-image~=1.1 (from google-images-search)\n",
      "  Downloading python_resize_image-1.1.20-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting google-api-python-client~=2.48.0 (from google-images-search)\n",
      "  Downloading google_api_python_client-2.48.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.48.0->google-images-search) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.48.0->google-images-search) (2.27.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.48.0->google-images-search) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.48.0->google-images-search) (1.34.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.48.0->google-images-search) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.21->google-images-search) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.21->google-images-search) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.21->google-images-search) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.21->google-images-search) (2025.1.31)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client~=2.48.0->google-images-search) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client~=2.48.0->google-images-search) (3.20.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client~=2.48.0->google-images-search) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client~=2.48.0->google-images-search) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client~=2.48.0->google-images-search) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client~=2.48.0->google-images-search) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.16.0->google-api-python-client~=2.48.0->google-images-search) (0.6.1)\n",
      "Downloading Google_Images_Search-1.4.7-py2.py3-none-any.whl (12 kB)\n",
      "Downloading google_api_python_client-2.48.0-py2.py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyfiglet-0.8.post1-py2.py3-none-any.whl (865 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.8/865.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_resize_image-1.1.20-py2.py3-none-any.whl (8.4 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=b1c933181392ccada7928d54df19ff433686c751924576008bbb220d8fa800e1\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
      "Successfully built termcolor\n",
      "Installing collected packages: termcolor, pyfiglet, python-resize-image, google-api-python-client, google-images-search\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.5.0\n",
      "    Uninstalling termcolor-2.5.0:\n",
      "      Successfully uninstalled termcolor-2.5.0\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.155.0\n",
      "    Uninstalling google-api-python-client-2.155.0:\n",
      "      Successfully uninstalled google-api-python-client-2.155.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.1 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-python-client-2.48.0 google-images-search-1.4.7 pyfiglet-0.8.post1 python-resize-image-1.1.20 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install google-images-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T17:51:25.807736Z",
     "iopub.status.busy": "2025-03-07T17:51:25.807226Z",
     "iopub.status.idle": "2025-03-07T17:51:25.813139Z",
     "shell.execute_reply": "2025-03-07T17:51:25.811871Z",
     "shell.execute_reply.started": "2025-03-07T17:51:25.807704Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder not found, skipping deletion.\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/kaggle/working/image_dataset2\"  # Folder to delete\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"Deleted folder: {folder_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Folder not found, skipping deletion.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting folder: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T17:51:40.917740Z",
     "iopub.status.busy": "2025-03-07T17:51:40.917384Z",
     "iopub.status.idle": "2025-03-07T17:54:45.136902Z",
     "shell.execute_reply": "2025-03-07T17:54:45.136015Z",
     "shell.execute_reply.started": "2025-03-07T17:51:40.917709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching images for: Backpack\n",
      "Fetching images for: Umbrella\n",
      "Fetching images for: Headphone\n",
      "Fetching images for: Laptop\n",
      "Fetching images for: Sunglasse\n",
      "Dataset collection completed.\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual API Key & CX ID (remove spaces)\n",
    "GCS_DEVELOPER_KEY = \"your_API_key\"\n",
    "GCS_CX = \"Your_product_id\"\n",
    "\n",
    "gis = GoogleImagesSearch(GCS_DEVELOPER_KEY, GCS_CX)\n",
    "\n",
    "# Define categories\n",
    "categories = [\n",
    "    \"Backpack\", \"Umbrella\", \"Headphone\", \"Laptop\", \"Sunglasse\"\n",
    "]\n",
    "\n",
    "# Create dataset directory\n",
    "dataset_dir = \"image_dataset\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# CSV file to store metadata\n",
    "csv_file = os.path.join(dataset_dir, \"metadata.csv\")\n",
    "\n",
    "with open(csv_file, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Category\", \"Image Name\", \"Image URL\", \"Resolution\"])\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Fetching images for: {category}\")\n",
    "        category_dir = os.path.join(dataset_dir, category)\n",
    "        os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "        # Download images in batches to stay within Google's limit\n",
    "        num_images = 40  # Total images per category\n",
    "        batch_size = 10  # Number of images per request\n",
    "        image_counter = 1  # Counter to ensure sequential naming\n",
    "\n",
    "        for start in range(1, min(num_images, 91), batch_size):  # Limit `start` to 91 max\n",
    "            gis.search({\n",
    "                'q': category,\n",
    "                'num': batch_size,\n",
    "                'start': start,\n",
    "                'searchType': 'image'\n",
    "            })\n",
    "\n",
    "            for image in gis.results():\n",
    "                img_url = image.url\n",
    "                img_name = f\"{category}_{image_counter}.jpg\"\n",
    "                img_path = os.path.join(category_dir, img_name)\n",
    "\n",
    "                # Download image and get its actual saved path\n",
    "                image.download(category_dir)\n",
    "                downloaded_images = sorted(\n",
    "                    [f for f in os.listdir(category_dir) if os.path.isfile(os.path.join(category_dir, f))],\n",
    "                    key=lambda x: os.path.getctime(os.path.join(category_dir, x))\n",
    "                )\n",
    "\n",
    "                if downloaded_images:\n",
    "                    downloaded_image_path = os.path.join(category_dir, downloaded_images[-1])  # Latest downloaded file\n",
    "                    new_img_path = os.path.join(category_dir, img_name)\n",
    "                    shutil.move(downloaded_image_path, new_img_path)\n",
    "\n",
    "                    # Store metadata\n",
    "                    writer.writerow([category, img_name, img_url, \"N/A\"])  # Resolution not available\n",
    "\n",
    "                image_counter += 1  # Increment counter for next image\n",
    "\n",
    "print(\"Dataset collection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T17:54:49.747875Z",
     "iopub.status.busy": "2025-03-07T17:54:49.747519Z",
     "iopub.status.idle": "2025-03-07T17:58:57.372546Z",
     "shell.execute_reply": "2025-03-07T17:58:57.371323Z",
     "shell.execute_reply.started": "2025-03-07T17:54:49.747847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching images for: Sneakers\n",
      "Fetching images for: Hoodie\n",
      "Fetching images for: Handbag\n",
      "Fetching images for: Wristwatche\n",
      "Fetching images for: Hat\n",
      "Dataset collection completed.\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual API Key & CX ID (remove spaces)\n",
    "GCS_DEVELOPER_KEY = \"your_API_key\"\n",
    "GCS_CX = \"Your_product_id\"\n",
    "\n",
    "gis = GoogleImagesSearch(GCS_DEVELOPER_KEY, GCS_CX)\n",
    "\n",
    "# Define categories\n",
    "categories = [\n",
    "    \"Sneakers\", \"Hoodie\", \"Handbag\", \"Wristwatche\", \"Hat\"\n",
    "]\n",
    "\n",
    "# Create dataset directory\n",
    "# dataset_dir = \"image_dataset\"\n",
    "# os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# CSV file to store metadata\n",
    "#csv_file = os.path.join(dataset_dir, \"metadata.csv\")\n",
    "\n",
    "with open(csv_file, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Category\", \"Image Name\", \"Image URL\", \"Resolution\"])\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Fetching images for: {category}\")\n",
    "        category_dir = os.path.join(dataset_dir, category)\n",
    "        os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "        # Download images in batches to stay within Google's limit\n",
    "        num_images = 40  # Total images per category\n",
    "        batch_size = 10  # Number of images per request\n",
    "        image_counter = 1  # Counter to ensure sequential naming\n",
    "\n",
    "        for start in range(1, min(num_images, 91), batch_size):  # Limit `start` to 91 max\n",
    "            gis.search({\n",
    "                'q': category,\n",
    "                'num': batch_size,\n",
    "                'start': start,\n",
    "                'searchType': 'image'\n",
    "            })\n",
    "\n",
    "            for image in gis.results():\n",
    "                img_url = image.url\n",
    "                img_name = f\"{category}_{image_counter}.jpg\"\n",
    "                img_path = os.path.join(category_dir, img_name)\n",
    "\n",
    "                # Download image and get its actual saved path\n",
    "                image.download(category_dir)\n",
    "                downloaded_images = sorted(\n",
    "                    [f for f in os.listdir(category_dir) if os.path.isfile(os.path.join(category_dir, f))],\n",
    "                    key=lambda x: os.path.getctime(os.path.join(category_dir, x))\n",
    "                )\n",
    "\n",
    "                if downloaded_images:\n",
    "                    downloaded_image_path = os.path.join(category_dir, downloaded_images[-1])  # Latest downloaded file\n",
    "                    new_img_path = os.path.join(category_dir, img_name)\n",
    "                    shutil.move(downloaded_image_path, new_img_path)\n",
    "\n",
    "                    # Store metadata\n",
    "                    writer.writerow([category, img_name, img_url, \"N/A\"])  # Resolution not available\n",
    "\n",
    "                image_counter += 1  # Increment counter for next image\n",
    "\n",
    "print(\"Dataset collection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T17:59:41.828798Z",
     "iopub.status.busy": "2025-03-07T17:59:41.828461Z",
     "iopub.status.idle": "2025-03-07T18:07:16.289975Z",
     "shell.execute_reply": "2025-03-07T18:07:16.288885Z",
     "shell.execute_reply.started": "2025-03-07T17:59:41.828771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching images for: Pizza\n",
      "Fetching images for: Burger\n",
      "Fetching images for: Ice Cream\n",
      "Fetching images for: Coffee Cup\n",
      "Fetching images for: Sushi\n",
      "Dataset collection completed.\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual API Key & CX ID (remove spaces)\n",
    "GCS_DEVELOPER_KEY = \"your_API_key\"\n",
    "GCS_CX = \"Your_product_id\"\n",
    "\n",
    "gis = GoogleImagesSearch(GCS_DEVELOPER_KEY, GCS_CX)\n",
    "\n",
    "# Define categories\n",
    "categories = [\n",
    "    \"Pizza\", \"Burger\", \"Ice Cream\", \"Coffee Cup\", \"Sushi\"\n",
    "]\n",
    "\n",
    "# Create dataset directory\n",
    "# dataset_dir = \"image_dataset\"\n",
    "# os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# CSV file to store metadata\n",
    "#csv_file = os.path.join(dataset_dir, \"metadata.csv\")\n",
    "\n",
    "with open(csv_file, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Category\", \"Image Name\", \"Image URL\", \"Resolution\"])\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Fetching images for: {category}\")\n",
    "        category_dir = os.path.join(dataset_dir, category)\n",
    "        os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "        # Download images in batches to stay within Google's limit\n",
    "        num_images = 40  # Total images per category\n",
    "        batch_size = 10  # Number of images per request\n",
    "        image_counter = 1  # Counter to ensure sequential naming\n",
    "\n",
    "        for start in range(1, min(num_images, 91), batch_size):  # Limit `start` to 91 max\n",
    "            gis.search({\n",
    "                'q': category,\n",
    "                'num': batch_size,\n",
    "                'start': start,\n",
    "                'searchType': 'image'\n",
    "            })\n",
    "\n",
    "            for image in gis.results():\n",
    "                img_url = image.url\n",
    "                img_name = f\"{category}_{image_counter}.jpg\"\n",
    "                img_path = os.path.join(category_dir, img_name)\n",
    "\n",
    "                # Download image and get its actual saved path\n",
    "                image.download(category_dir)\n",
    "                downloaded_images = sorted(\n",
    "                    [f for f in os.listdir(category_dir) if os.path.isfile(os.path.join(category_dir, f))],\n",
    "                    key=lambda x: os.path.getctime(os.path.join(category_dir, x))\n",
    "                )\n",
    "\n",
    "                if downloaded_images:\n",
    "                    downloaded_image_path = os.path.join(category_dir, downloaded_images[-1])  # Latest downloaded file\n",
    "                    new_img_path = os.path.join(category_dir, img_name)\n",
    "                    shutil.move(downloaded_image_path, new_img_path)\n",
    "\n",
    "                    # Store metadata\n",
    "                    writer.writerow([category, img_name, img_url, \"N/A\"])  # Resolution not available\n",
    "\n",
    "                image_counter += 1  # Increment counter for next image\n",
    "\n",
    "print(\"Dataset collection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T18:09:07.696266Z",
     "iopub.status.busy": "2025-03-07T18:09:07.695878Z",
     "iopub.status.idle": "2025-03-07T18:14:45.691725Z",
     "shell.execute_reply": "2025-03-07T18:14:45.689963Z",
     "shell.execute_reply.started": "2025-03-07T18:09:07.696235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching images for: Bicycle\n",
      "Fetching images for: Motorcycle\n",
      "Fetching images for: Truck\n",
      "Fetching images for: Electric Car\n",
      "Fetching images for: Street Sign\n",
      "Dataset collection completed.\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual API Key & CX ID (remove spaces)\n",
    "GCS_DEVELOPER_KEY = \"your_API_key\"\n",
    "GCS_CX = \"Your_product_id\"\n",
    "\n",
    "gis = GoogleImagesSearch(GCS_DEVELOPER_KEY, GCS_CX)\n",
    "\n",
    "# Define categories\n",
    "categories = [\n",
    "    \"Bicycle\", \"Motorcycle\", \"Truck\", \"Electric Car\", \"Street Sign\"\n",
    "]\n",
    "\n",
    "# Create dataset directory\n",
    "# dataset_dir = \"image_dataset\"\n",
    "# os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# CSV file to store metadata\n",
    "#csv_file = os.path.join(dataset_dir, \"metadata.csv\")\n",
    "\n",
    "with open(csv_file, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Category\", \"Image Name\", \"Image URL\", \"Resolution\"])\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Fetching images for: {category}\")\n",
    "        category_dir = os.path.join(dataset_dir, category)\n",
    "        os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "        # Download images in batches to stay within Google's limit\n",
    "        num_images = 40  # Total images per category\n",
    "        batch_size = 10  # Number of images per request\n",
    "        image_counter = 1  # Counter to ensure sequential naming\n",
    "\n",
    "        for start in range(1, min(num_images, 91), batch_size):  # Limit `start` to 91 max\n",
    "            gis.search({\n",
    "                'q': category,\n",
    "                'num': batch_size,\n",
    "                'start': start,\n",
    "                'searchType': 'image'\n",
    "            })\n",
    "\n",
    "            for image in gis.results():\n",
    "                img_url = image.url\n",
    "                img_name = f\"{category}_{image_counter}.jpg\"\n",
    "                img_path = os.path.join(category_dir, img_name)\n",
    "\n",
    "                # Download image and get its actual saved path\n",
    "                image.download(category_dir)\n",
    "                downloaded_images = sorted(\n",
    "                    [f for f in os.listdir(category_dir) if os.path.isfile(os.path.join(category_dir, f))],\n",
    "                    key=lambda x: os.path.getctime(os.path.join(category_dir, x))\n",
    "                )\n",
    "\n",
    "                if downloaded_images:\n",
    "                    downloaded_image_path = os.path.join(category_dir, downloaded_images[-1])  # Latest downloaded file\n",
    "                    new_img_path = os.path.join(category_dir, img_name)\n",
    "                    shutil.move(downloaded_image_path, new_img_path)\n",
    "\n",
    "                    # Store metadata\n",
    "                    writer.writerow([category, img_name, img_url, \"N/A\"])  # Resolution not available\n",
    "\n",
    "                image_counter += 1  # Increment counter for next image\n",
    "\n",
    "print(\"Dataset collection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T18:19:36.097120Z",
     "iopub.status.busy": "2025-03-07T18:19:36.096799Z",
     "iopub.status.idle": "2025-03-07T18:19:36.109632Z",
     "shell.execute_reply": "2025-03-07T18:19:36.107990Z",
     "shell.execute_reply.started": "2025-03-07T18:19:36.097092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test dataset created with 3 images per category.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define dataset paths\n",
    "dataset_dir = \"image_dataset\"\n",
    "test_dir = \"test_dataset\"\n",
    "\n",
    "# Create test dataset directory\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Move 3 images per category to test dataset\n",
    "for category in os.listdir(dataset_dir):\n",
    "    category_path = os.path.join(dataset_dir, category)\n",
    "    test_category_path = os.path.join(test_dir, category)\n",
    "    \n",
    "    if os.path.isdir(category_path):\n",
    "        os.makedirs(test_category_path, exist_ok=True)\n",
    "\n",
    "        # Select 3 random images\n",
    "        images = [img for img in os.listdir(category_path) if img.endswith(('.jpg', '.png'))]\n",
    "        test_images = random.sample(images, 3)\n",
    "\n",
    "        for img in test_images:\n",
    "            src_path = os.path.join(category_path, img)\n",
    "            dest_path = os.path.join(test_category_path, img)\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "print(\"✅ Test dataset created with 3 images per category.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T18:24:05.477705Z",
     "iopub.status.busy": "2025-03-07T18:24:05.477354Z",
     "iopub.status.idle": "2025-03-07T18:24:07.300547Z",
     "shell.execute_reply": "2025-03-07T18:24:07.299355Z",
     "shell.execute_reply.started": "2025-03-07T18:24:05.477677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing corrupted image: image_dataset/Electric Car/Electric Car_3.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "dataset_dir = \"image_dataset\"  # Update with your dataset path\n",
    "\n",
    "for category in os.listdir(dataset_dir):\n",
    "    category_path = os.path.join(dataset_dir, category)\n",
    "    if os.path.isdir(category_path):  # Ensure it's a folder\n",
    "        for img_name in os.listdir(category_path):\n",
    "            img_path = os.path.join(category_path, img_name)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()  # Verify the image is valid\n",
    "            except (IOError, SyntaxError):\n",
    "                print(f\"Removing corrupted image: {img_path}\")\n",
    "                os.remove(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T18:24:14.672813Z",
     "iopub.status.busy": "2025-03-07T18:24:14.672461Z",
     "iopub.status.idle": "2025-03-07T18:41:32.301394Z",
     "shell.execute_reply": "2025-03-07T18:41:32.300275Z",
     "shell.execute_reply.started": "2025-03-07T18:24:14.672781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 21 categories.\n",
      "Found 739 images belonging to 20 classes.\n",
      "Found 60 images belonging to 20 classes.\n",
      "Epoch 1/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 701ms/step - accuracy: 0.0653 - loss: 3.1031 - val_accuracy: 0.0667 - val_loss: 2.9782\n",
      "Epoch 2/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 635ms/step - accuracy: 0.0758 - loss: 2.9945 - val_accuracy: 0.1167 - val_loss: 2.9584\n",
      "Epoch 3/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 694ms/step - accuracy: 0.0817 - loss: 2.9614 - val_accuracy: 0.1167 - val_loss: 2.9406\n",
      "Epoch 4/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 692ms/step - accuracy: 0.1088 - loss: 2.9362 - val_accuracy: 0.0833 - val_loss: 2.9154\n",
      "Epoch 5/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 671ms/step - accuracy: 0.1075 - loss: 2.9073 - val_accuracy: 0.1000 - val_loss: 2.9085\n",
      "Epoch 6/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 666ms/step - accuracy: 0.1285 - loss: 2.8854 - val_accuracy: 0.1333 - val_loss: 2.8742\n",
      "Epoch 7/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 707ms/step - accuracy: 0.1129 - loss: 2.8850 - val_accuracy: 0.1500 - val_loss: 2.8536\n",
      "Epoch 8/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 678ms/step - accuracy: 0.1159 - loss: 2.8681 - val_accuracy: 0.1333 - val_loss: 2.8367\n",
      "Epoch 9/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 690ms/step - accuracy: 0.1536 - loss: 2.8081 - val_accuracy: 0.1833 - val_loss: 2.8021\n",
      "Epoch 10/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 672ms/step - accuracy: 0.1212 - loss: 2.8111 - val_accuracy: 0.1500 - val_loss: 2.7742\n",
      "Epoch 11/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 690ms/step - accuracy: 0.1447 - loss: 2.7994 - val_accuracy: 0.1500 - val_loss: 2.7523\n",
      "Epoch 12/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 728ms/step - accuracy: 0.1732 - loss: 2.7318 - val_accuracy: 0.1333 - val_loss: 2.7265\n",
      "Epoch 13/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 692ms/step - accuracy: 0.1614 - loss: 2.7372 - val_accuracy: 0.2167 - val_loss: 2.6939\n",
      "Epoch 14/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 684ms/step - accuracy: 0.1406 - loss: 2.6959 - val_accuracy: 0.2167 - val_loss: 2.6755\n",
      "Epoch 15/15\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 677ms/step - accuracy: 0.1470 - loss: 2.7299 - val_accuracy: 0.2000 - val_loss: 2.6452\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 487ms/step - accuracy: 0.2465 - loss: 2.5938\n",
      "✅ Test Accuracy: 0.2000\n",
      "✅ Model saved as 'image_classifier_resnet.h5'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set dataset paths\n",
    "train_dir = \"image_dataset\"\n",
    "test_dir = \"test_dataset\"\n",
    "\n",
    "# Get number of categories dynamically\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "print(f\"Detected {num_classes} categories.\")\n",
    "\n",
    "# Image parameters\n",
    "IMG_SIZE = (128, 128)  # ResNet50 input size\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Only rescale for testing (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Load training and testing data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load Pretrained ResNet50 Model (without top layers)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False  # Freeze pretrained layers\n",
    "\n",
    "# Add custom classifier on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Reduce dimensions\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)  # Prevent overfitting\n",
    "x = Dense(num_classes-1, activation='softmax')(x)  # Output layer\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "EPOCHS = 15\n",
    "history = model.fit(train_generator, epochs=EPOCHS, validation_data=test_generator, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate model on test data\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"✅ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Save trained model\n",
    "model.save(\"image_classifier_resnet.h5\")\n",
    "print(\"✅ Model saved as 'image_classifier_resnet.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T18:44:03.378806Z",
     "iopub.status.busy": "2025-03-07T18:44:03.378258Z",
     "iopub.status.idle": "2025-03-07T18:44:58.422012Z",
     "shell.execute_reply": "2025-03-07T18:44:58.420043Z",
     "shell.execute_reply.started": "2025-03-07T18:44:03.378726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/image_dataset_export.zip'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define dataset path\n",
    "dataset_dir = \"/kaggle/working/image_dataset\"\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "shutil.make_archive(\"/kaggle/working/image_dataset_export\", 'zip', dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T18:45:31.439331Z",
     "iopub.status.busy": "2025-03-07T18:45:31.438487Z",
     "iopub.status.idle": "2025-03-07T18:45:34.249128Z",
     "shell.execute_reply": "2025-03-07T18:45:34.247412Z",
     "shell.execute_reply.started": "2025-03-07T18:45:31.439293Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/kaggle/working/test_dataset.zip'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define dataset path\n",
    "dataset_dir = \"/kaggle/working/test_dataset\"\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "shutil.make_archive(\"/kaggle/working//kaggle/working/test_dataset\", 'zip', dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T19:00:19.970361Z",
     "iopub.status.busy": "2025-03-07T19:00:19.970017Z",
     "iopub.status.idle": "2025-03-07T19:00:19.997376Z",
     "shell.execute_reply": "2025-03-07T19:00:19.996038Z",
     "shell.execute_reply.started": "2025-03-07T19:00:19.970334Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted folder: /kaggle/working/kaggle\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "folder_path = \"/kaggle/working/kaggle\"  # Folder to delete\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"Deleted folder: {folder_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Folder not found, skipping deletion.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting folder: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T19:03:22.067568Z",
     "iopub.status.busy": "2025-03-07T19:03:22.067202Z",
     "iopub.status.idle": "2025-03-07T19:03:39.669298Z",
     "shell.execute_reply": "2025-03-07T19:03:39.667836Z",
     "shell.execute_reply.started": "2025-03-07T19:03:22.067539Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset filtered & zipped successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define paths\n",
    "original_dataset_dir = \"/kaggle/working/image_dataset\"  # Update this if needed\n",
    "filtered_dataset_dir = \"/kaggle/working/filtered_image_dataset\"\n",
    "os.makedirs(filtered_dataset_dir, exist_ok=True)\n",
    "\n",
    "# Get category folders\n",
    "categories = [d for d in os.listdir(original_dataset_dir) if os.path.isdir(os.path.join(original_dataset_dir, d))]\n",
    "\n",
    "for category in categories:\n",
    "    category_path = os.path.join(original_dataset_dir, category)\n",
    "    new_category_path = os.path.join(filtered_dataset_dir, category)\n",
    "    os.makedirs(new_category_path, exist_ok=True)\n",
    "\n",
    "    # Select 10 random images\n",
    "    images = [f for f in os.listdir(category_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    selected_images = random.sample(images, min(10, len(images)))  # Ensure at most 10 images\n",
    "\n",
    "    # Copy selected images to new folder\n",
    "    for image in selected_images:\n",
    "        shutil.copy(os.path.join(category_path, image), os.path.join(new_category_path, image))\n",
    "\n",
    "# Zip the dataset\n",
    "shutil.make_archive(\"/kaggle/working/filtered_image_dataset_export\", 'zip', filtered_dataset_dir)\n",
    "\n",
    "print(\"✅ Dataset filtered & zipped successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T19:05:07.349985Z",
     "iopub.status.busy": "2025-03-07T19:05:07.349590Z",
     "iopub.status.idle": "2025-03-07T19:05:07.385548Z",
     "shell.execute_reply": "2025-03-07T19:05:07.383928Z",
     "shell.execute_reply.started": "2025-03-07T19:05:07.349953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted folder: /kaggle/working/test_dataset\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "folder_path = \"/kaggle/working/test_dataset\"  # Folder to delete\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"Deleted folder: {folder_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Folder not found, skipping deletion.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting folder: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
