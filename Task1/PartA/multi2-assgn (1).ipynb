{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport csv\nfrom google_images_search import GoogleImagesSearch\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:51:14.064510Z","iopub.execute_input":"2025-03-07T17:51:14.064912Z","iopub.status.idle":"2025-03-07T17:51:15.326498Z","shell.execute_reply.started":"2025-03-07T17:51:14.064882Z","shell.execute_reply":"2025-03-07T17:51:15.324968Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install google-images-search","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:50:57.151232Z","iopub.execute_input":"2025-03-07T17:50:57.151577Z","iopub.status.idle":"2025-03-07T17:51:10.220468Z","shell.execute_reply.started":"2025-03-07T17:50:57.151550Z","shell.execute_reply":"2025-03-07T17:51:10.218780Z"}},"outputs":[{"name":"stdout","text":"Collecting google-images-search\n  Downloading Google_Images_Search-1.4.7-py2.py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: colorama~=0.4 in /usr/local/lib/python3.10/dist-packages (from google-images-search) (0.4.6)\nCollecting pyfiglet~=0.8 (from google-images-search)\n  Downloading pyfiglet-0.8.post1-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting termcolor~=1.1 (from google-images-search)\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: click<=8.2,>=7.0 in /usr/local/lib/python3.10/dist-packages (from google-images-search) (8.1.7)\nRequirement already satisfied: six~=1.12 in /usr/local/lib/python3.10/dist-packages (from google-images-search) (1.17.0)\nRequirement already satisfied: requests~=2.21 in /usr/local/lib/python3.10/dist-packages (from google-images-search) (2.32.3)\nRequirement already satisfied: Pillow>=8.1.1 in /usr/local/lib/python3.10/dist-packages (from google-images-search) (11.0.0)\nCollecting python-resize-image~=1.1 (from google-images-search)\n  Downloading python_resize_image-1.1.20-py2.py3-none-any.whl.metadata (9.0 kB)\nCollecting google-api-python-client~=2.48.0 (from google-images-search)\n  Downloading google_api_python_client-2.48.0-py2.py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.48.0->google-images-search) (0.22.0)\nRequirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.48.0->google-images-search) (2.27.0)\nRequirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.48.0->google-images-search) (0.2.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.48.0->google-images-search) (1.34.1)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.48.0->google-images-search) (4.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.21->google-images-search) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.21->google-images-search) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.21->google-images-search) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.21->google-images-search) (2025.1.31)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client~=2.48.0->google-images-search) (1.66.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client~=2.48.0->google-images-search) (3.20.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client~=2.48.0->google-images-search) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client~=2.48.0->google-images-search) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client~=2.48.0->google-images-search) (4.9)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client~=2.48.0->google-images-search) (3.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.16.0->google-api-python-client~=2.48.0->google-images-search) (0.6.1)\nDownloading Google_Images_Search-1.4.7-py2.py3-none-any.whl (12 kB)\nDownloading google_api_python_client-2.48.0-py2.py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyfiglet-0.8.post1-py2.py3-none-any.whl (865 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.8/865.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_resize_image-1.1.20-py2.py3-none-any.whl (8.4 kB)\nBuilding wheels for collected packages: termcolor\n  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=b1c933181392ccada7928d54df19ff433686c751924576008bbb220d8fa800e1\n  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\nSuccessfully built termcolor\nInstalling collected packages: termcolor, pyfiglet, python-resize-image, google-api-python-client, google-images-search\n  Attempting uninstall: termcolor\n    Found existing installation: termcolor 2.5.0\n    Uninstalling termcolor-2.5.0:\n      Successfully uninstalled termcolor-2.5.0\n  Attempting uninstall: google-api-python-client\n    Found existing installation: google-api-python-client 2.155.0\n    Uninstalling google-api-python-client-2.155.0:\n      Successfully uninstalled google-api-python-client-2.155.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed google-api-python-client-2.48.0 google-images-search-1.4.7 pyfiglet-0.8.post1 python-resize-image-1.1.20 termcolor-1.1.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"folder_path = \"/kaggle/working/image_dataset2\"  # Folder to delete\n\ntry:\n    shutil.rmtree(folder_path)\n    print(f\"Deleted folder: {folder_path}\")\nexcept FileNotFoundError:\n    print(\"Folder not found, skipping deletion.\")\nexcept Exception as e:\n    print(f\"Error deleting folder: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:51:25.807226Z","iopub.execute_input":"2025-03-07T17:51:25.807736Z","iopub.status.idle":"2025-03-07T17:51:25.813139Z","shell.execute_reply.started":"2025-03-07T17:51:25.807704Z","shell.execute_reply":"2025-03-07T17:51:25.811871Z"}},"outputs":[{"name":"stdout","text":"Folder not found, skipping deletion.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Replace with your actual API Key & CX ID (remove spaces)\nGCS_DEVELOPER_KEY = \"AIzaSyDlEKHbgwpLv8Kk1RWAP5sU1FhY1CYUW_o\"\nGCS_CX = \"c0826223ccd2c4c4d\"\n\ngis = GoogleImagesSearch(GCS_DEVELOPER_KEY, GCS_CX)\n\n# Define categories\ncategories = [\n    \"Backpack\", \"Umbrella\", \"Headphone\", \"Laptop\", \"Sunglasse\"\n]\n\n# Create dataset directory\ndataset_dir = \"image_dataset\"\nos.makedirs(dataset_dir, exist_ok=True)\n\n# CSV file to store metadata\ncsv_file = os.path.join(dataset_dir, \"metadata.csv\")\n\nwith open(csv_file, \"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Category\", \"Image Name\", \"Image URL\", \"Resolution\"])\n\n    for category in categories:\n        print(f\"Fetching images for: {category}\")\n        category_dir = os.path.join(dataset_dir, category)\n        os.makedirs(category_dir, exist_ok=True)\n\n        # Download images in batches to stay within Google's limit\n        num_images = 40  # Total images per category\n        batch_size = 10  # Number of images per request\n        image_counter = 1  # Counter to ensure sequential naming\n\n        for start in range(1, min(num_images, 91), batch_size):  # Limit `start` to 91 max\n            gis.search({\n                'q': category,\n                'num': batch_size,\n                'start': start,\n                'searchType': 'image'\n            })\n\n            for image in gis.results():\n                img_url = image.url\n                img_name = f\"{category}_{image_counter}.jpg\"\n                img_path = os.path.join(category_dir, img_name)\n\n                # Download image and get its actual saved path\n                image.download(category_dir)\n                downloaded_images = sorted(\n                    [f for f in os.listdir(category_dir) if os.path.isfile(os.path.join(category_dir, f))],\n                    key=lambda x: os.path.getctime(os.path.join(category_dir, x))\n                )\n\n                if downloaded_images:\n                    downloaded_image_path = os.path.join(category_dir, downloaded_images[-1])  # Latest downloaded file\n                    new_img_path = os.path.join(category_dir, img_name)\n                    shutil.move(downloaded_image_path, new_img_path)\n\n                    # Store metadata\n                    writer.writerow([category, img_name, img_url, \"N/A\"])  # Resolution not available\n\n                image_counter += 1  # Increment counter for next image\n\nprint(\"Dataset collection completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:51:40.917384Z","iopub.execute_input":"2025-03-07T17:51:40.917740Z","iopub.status.idle":"2025-03-07T17:54:45.136902Z","shell.execute_reply.started":"2025-03-07T17:51:40.917709Z","shell.execute_reply":"2025-03-07T17:54:45.136015Z"}},"outputs":[{"name":"stdout","text":"Fetching images for: Backpack\nFetching images for: Umbrella\nFetching images for: Headphone\nFetching images for: Laptop\nFetching images for: Sunglasse\nDataset collection completed.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Replace with your actual API Key & CX ID (remove spaces)\nGCS_DEVELOPER_KEY = \"AIzaSyDpGmG68YlWm3hNqzYrX0Xn24QG5C6KK6U\"\nGCS_CX = \"c0826223ccd2c4c4d\"\n\ngis = GoogleImagesSearch(GCS_DEVELOPER_KEY, GCS_CX)\n\n# Define categories\ncategories = [\n    \"Sneakers\", \"Hoodie\", \"Handbag\", \"Wristwatche\", \"Hat\"\n]\n\n# Create dataset directory\n# dataset_dir = \"image_dataset\"\n# os.makedirs(dataset_dir, exist_ok=True)\n\n# CSV file to store metadata\n#csv_file = os.path.join(dataset_dir, \"metadata.csv\")\n\nwith open(csv_file, \"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Category\", \"Image Name\", \"Image URL\", \"Resolution\"])\n\n    for category in categories:\n        print(f\"Fetching images for: {category}\")\n        category_dir = os.path.join(dataset_dir, category)\n        os.makedirs(category_dir, exist_ok=True)\n\n        # Download images in batches to stay within Google's limit\n        num_images = 40  # Total images per category\n        batch_size = 10  # Number of images per request\n        image_counter = 1  # Counter to ensure sequential naming\n\n        for start in range(1, min(num_images, 91), batch_size):  # Limit `start` to 91 max\n            gis.search({\n                'q': category,\n                'num': batch_size,\n                'start': start,\n                'searchType': 'image'\n            })\n\n            for image in gis.results():\n                img_url = image.url\n                img_name = f\"{category}_{image_counter}.jpg\"\n                img_path = os.path.join(category_dir, img_name)\n\n                # Download image and get its actual saved path\n                image.download(category_dir)\n                downloaded_images = sorted(\n                    [f for f in os.listdir(category_dir) if os.path.isfile(os.path.join(category_dir, f))],\n                    key=lambda x: os.path.getctime(os.path.join(category_dir, x))\n                )\n\n                if downloaded_images:\n                    downloaded_image_path = os.path.join(category_dir, downloaded_images[-1])  # Latest downloaded file\n                    new_img_path = os.path.join(category_dir, img_name)\n                    shutil.move(downloaded_image_path, new_img_path)\n\n                    # Store metadata\n                    writer.writerow([category, img_name, img_url, \"N/A\"])  # Resolution not available\n\n                image_counter += 1  # Increment counter for next image\n\nprint(\"Dataset collection completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:54:49.747519Z","iopub.execute_input":"2025-03-07T17:54:49.747875Z","iopub.status.idle":"2025-03-07T17:58:57.372546Z","shell.execute_reply.started":"2025-03-07T17:54:49.747847Z","shell.execute_reply":"2025-03-07T17:58:57.371323Z"}},"outputs":[{"name":"stdout","text":"Fetching images for: Sneakers\nFetching images for: Hoodie\nFetching images for: Handbag\nFetching images for: Wristwatche\nFetching images for: Hat\nDataset collection completed.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Replace with your actual API Key & CX ID (remove spaces)\nGCS_DEVELOPER_KEY = \"AIzaSyDu-DRH7OrlVsT1F2g1VF0Kv4tX6eG3VVM\"\nGCS_CX = \"c0826223ccd2c4c4d\"\n\ngis = GoogleImagesSearch(GCS_DEVELOPER_KEY, GCS_CX)\n\n# Define categories\ncategories = [\n    \"Pizza\", \"Burger\", \"Ice Cream\", \"Coffee Cup\", \"Sushi\"\n]\n\n# Create dataset directory\n# dataset_dir = \"image_dataset\"\n# os.makedirs(dataset_dir, exist_ok=True)\n\n# CSV file to store metadata\n#csv_file = os.path.join(dataset_dir, \"metadata.csv\")\n\nwith open(csv_file, \"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Category\", \"Image Name\", \"Image URL\", \"Resolution\"])\n\n    for category in categories:\n        print(f\"Fetching images for: {category}\")\n        category_dir = os.path.join(dataset_dir, category)\n        os.makedirs(category_dir, exist_ok=True)\n\n        # Download images in batches to stay within Google's limit\n        num_images = 40  # Total images per category\n        batch_size = 10  # Number of images per request\n        image_counter = 1  # Counter to ensure sequential naming\n\n        for start in range(1, min(num_images, 91), batch_size):  # Limit `start` to 91 max\n            gis.search({\n                'q': category,\n                'num': batch_size,\n                'start': start,\n                'searchType': 'image'\n            })\n\n            for image in gis.results():\n                img_url = image.url\n                img_name = f\"{category}_{image_counter}.jpg\"\n                img_path = os.path.join(category_dir, img_name)\n\n                # Download image and get its actual saved path\n                image.download(category_dir)\n                downloaded_images = sorted(\n                    [f for f in os.listdir(category_dir) if os.path.isfile(os.path.join(category_dir, f))],\n                    key=lambda x: os.path.getctime(os.path.join(category_dir, x))\n                )\n\n                if downloaded_images:\n                    downloaded_image_path = os.path.join(category_dir, downloaded_images[-1])  # Latest downloaded file\n                    new_img_path = os.path.join(category_dir, img_name)\n                    shutil.move(downloaded_image_path, new_img_path)\n\n                    # Store metadata\n                    writer.writerow([category, img_name, img_url, \"N/A\"])  # Resolution not available\n\n                image_counter += 1  # Increment counter for next image\n\nprint(\"Dataset collection completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:59:41.828461Z","iopub.execute_input":"2025-03-07T17:59:41.828798Z","iopub.status.idle":"2025-03-07T18:07:16.289975Z","shell.execute_reply.started":"2025-03-07T17:59:41.828771Z","shell.execute_reply":"2025-03-07T18:07:16.288885Z"}},"outputs":[{"name":"stdout","text":"Fetching images for: Pizza\nFetching images for: Burger\nFetching images for: Ice Cream\nFetching images for: Coffee Cup\nFetching images for: Sushi\nDataset collection completed.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Replace with your actual API Key & CX ID (remove spaces)\nGCS_DEVELOPER_KEY = \"AIzaSyCMsxYxUrJXzpuJmo51eu_eaNkFBh4u7ZQ\"\nGCS_CX = \"c0826223ccd2c4c4d\"\n\ngis = GoogleImagesSearch(GCS_DEVELOPER_KEY, GCS_CX)\n\n# Define categories\ncategories = [\n    \"Bicycle\", \"Motorcycle\", \"Truck\", \"Electric Car\", \"Street Sign\"\n]\n\n# Create dataset directory\n# dataset_dir = \"image_dataset\"\n# os.makedirs(dataset_dir, exist_ok=True)\n\n# CSV file to store metadata\n#csv_file = os.path.join(dataset_dir, \"metadata.csv\")\n\nwith open(csv_file, \"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Category\", \"Image Name\", \"Image URL\", \"Resolution\"])\n\n    for category in categories:\n        print(f\"Fetching images for: {category}\")\n        category_dir = os.path.join(dataset_dir, category)\n        os.makedirs(category_dir, exist_ok=True)\n\n        # Download images in batches to stay within Google's limit\n        num_images = 40  # Total images per category\n        batch_size = 10  # Number of images per request\n        image_counter = 1  # Counter to ensure sequential naming\n\n        for start in range(1, min(num_images, 91), batch_size):  # Limit `start` to 91 max\n            gis.search({\n                'q': category,\n                'num': batch_size,\n                'start': start,\n                'searchType': 'image'\n            })\n\n            for image in gis.results():\n                img_url = image.url\n                img_name = f\"{category}_{image_counter}.jpg\"\n                img_path = os.path.join(category_dir, img_name)\n\n                # Download image and get its actual saved path\n                image.download(category_dir)\n                downloaded_images = sorted(\n                    [f for f in os.listdir(category_dir) if os.path.isfile(os.path.join(category_dir, f))],\n                    key=lambda x: os.path.getctime(os.path.join(category_dir, x))\n                )\n\n                if downloaded_images:\n                    downloaded_image_path = os.path.join(category_dir, downloaded_images[-1])  # Latest downloaded file\n                    new_img_path = os.path.join(category_dir, img_name)\n                    shutil.move(downloaded_image_path, new_img_path)\n\n                    # Store metadata\n                    writer.writerow([category, img_name, img_url, \"N/A\"])  # Resolution not available\n\n                image_counter += 1  # Increment counter for next image\n\nprint(\"Dataset collection completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:09:07.695878Z","iopub.execute_input":"2025-03-07T18:09:07.696266Z","iopub.status.idle":"2025-03-07T18:14:45.691725Z","shell.execute_reply.started":"2025-03-07T18:09:07.696235Z","shell.execute_reply":"2025-03-07T18:14:45.689963Z"}},"outputs":[{"name":"stdout","text":"Fetching images for: Bicycle\nFetching images for: Motorcycle\nFetching images for: Truck\nFetching images for: Electric Car\nFetching images for: Street Sign\nDataset collection completed.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# Define dataset paths\ndataset_dir = \"image_dataset\"\ntest_dir = \"test_dataset\"\n\n# Create test dataset directory\nos.makedirs(test_dir, exist_ok=True)\n\n# Move 3 images per category to test dataset\nfor category in os.listdir(dataset_dir):\n    category_path = os.path.join(dataset_dir, category)\n    test_category_path = os.path.join(test_dir, category)\n    \n    if os.path.isdir(category_path):\n        os.makedirs(test_category_path, exist_ok=True)\n\n        # Select 3 random images\n        images = [img for img in os.listdir(category_path) if img.endswith(('.jpg', '.png'))]\n        test_images = random.sample(images, 3)\n\n        for img in test_images:\n            src_path = os.path.join(category_path, img)\n            dest_path = os.path.join(test_category_path, img)\n            shutil.move(src_path, dest_path)\n\nprint(\"✅ Test dataset created with 3 images per category.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:19:36.096799Z","iopub.execute_input":"2025-03-07T18:19:36.097120Z","iopub.status.idle":"2025-03-07T18:19:36.109632Z","shell.execute_reply.started":"2025-03-07T18:19:36.097092Z","shell.execute_reply":"2025-03-07T18:19:36.107990Z"}},"outputs":[{"name":"stdout","text":"✅ Test dataset created with 3 images per category.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from PIL import Image\nimport os\n\ndataset_dir = \"image_dataset\"  # Update with your dataset path\n\nfor category in os.listdir(dataset_dir):\n    category_path = os.path.join(dataset_dir, category)\n    if os.path.isdir(category_path):  # Ensure it's a folder\n        for img_name in os.listdir(category_path):\n            img_path = os.path.join(category_path, img_name)\n            try:\n                with Image.open(img_path) as img:\n                    img.verify()  # Verify the image is valid\n            except (IOError, SyntaxError):\n                print(f\"Removing corrupted image: {img_path}\")\n                os.remove(img_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:24:05.477354Z","iopub.execute_input":"2025-03-07T18:24:05.477705Z","iopub.status.idle":"2025-03-07T18:24:07.300547Z","shell.execute_reply.started":"2025-03-07T18:24:05.477677Z","shell.execute_reply":"2025-03-07T18:24:07.299355Z"}},"outputs":[{"name":"stdout","text":"Removing corrupted image: image_dataset/Electric Car/Electric Car_3.jpg\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Set dataset paths\ntrain_dir = \"image_dataset\"\ntest_dir = \"test_dataset\"\n\n# Get number of categories dynamically\nnum_classes = len(os.listdir(train_dir))\nprint(f\"Detected {num_classes} categories.\")\n\n# Image parameters\nIMG_SIZE = (128, 128)  # ResNet50 input size\nBATCH_SIZE = 8\n\n# Data augmentation for training\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0/255.0,\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range=[0.8, 1.2],\n    fill_mode='nearest'\n)\n\n# Only rescale for testing (no augmentation)\ntest_datagen = ImageDataGenerator(rescale=1.0/255.0)\n\n# Load training and testing data\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Load Pretrained ResNet50 Model (without top layers)\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\nbase_model.trainable = False  # Freeze pretrained layers\n\n# Add custom classifier on top\nx = base_model.output\nx = GlobalAveragePooling2D()(x)  # Reduce dimensions\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.3)(x)  # Prevent overfitting\nx = Dense(num_classes-1, activation='softmax')(x)  # Output layer\n\n# Create model\nmodel = Model(inputs=base_model.input, outputs=x)\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=0.0005),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train model with early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nEPOCHS = 15\nhistory = model.fit(train_generator, epochs=EPOCHS, validation_data=test_generator, callbacks=[early_stopping])\n\n# Evaluate model on test data\ntest_loss, test_acc = model.evaluate(test_generator)\nprint(f\"✅ Test Accuracy: {test_acc:.4f}\")\n\n# Save trained model\nmodel.save(\"image_classifier_resnet.h5\")\nprint(\"✅ Model saved as 'image_classifier_resnet.h5'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:24:14.672461Z","iopub.execute_input":"2025-03-07T18:24:14.672813Z","iopub.status.idle":"2025-03-07T18:41:32.301394Z","shell.execute_reply.started":"2025-03-07T18:24:14.672781Z","shell.execute_reply":"2025-03-07T18:41:32.300275Z"}},"outputs":[{"name":"stdout","text":"Detected 21 categories.\nFound 739 images belonging to 20 classes.\nFound 60 images belonging to 20 classes.\nEpoch 1/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 701ms/step - accuracy: 0.0653 - loss: 3.1031 - val_accuracy: 0.0667 - val_loss: 2.9782\nEpoch 2/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 635ms/step - accuracy: 0.0758 - loss: 2.9945 - val_accuracy: 0.1167 - val_loss: 2.9584\nEpoch 3/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 694ms/step - accuracy: 0.0817 - loss: 2.9614 - val_accuracy: 0.1167 - val_loss: 2.9406\nEpoch 4/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 692ms/step - accuracy: 0.1088 - loss: 2.9362 - val_accuracy: 0.0833 - val_loss: 2.9154\nEpoch 5/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 671ms/step - accuracy: 0.1075 - loss: 2.9073 - val_accuracy: 0.1000 - val_loss: 2.9085\nEpoch 6/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 666ms/step - accuracy: 0.1285 - loss: 2.8854 - val_accuracy: 0.1333 - val_loss: 2.8742\nEpoch 7/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 707ms/step - accuracy: 0.1129 - loss: 2.8850 - val_accuracy: 0.1500 - val_loss: 2.8536\nEpoch 8/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 678ms/step - accuracy: 0.1159 - loss: 2.8681 - val_accuracy: 0.1333 - val_loss: 2.8367\nEpoch 9/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 690ms/step - accuracy: 0.1536 - loss: 2.8081 - val_accuracy: 0.1833 - val_loss: 2.8021\nEpoch 10/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 672ms/step - accuracy: 0.1212 - loss: 2.8111 - val_accuracy: 0.1500 - val_loss: 2.7742\nEpoch 11/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 690ms/step - accuracy: 0.1447 - loss: 2.7994 - val_accuracy: 0.1500 - val_loss: 2.7523\nEpoch 12/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 728ms/step - accuracy: 0.1732 - loss: 2.7318 - val_accuracy: 0.1333 - val_loss: 2.7265\nEpoch 13/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 692ms/step - accuracy: 0.1614 - loss: 2.7372 - val_accuracy: 0.2167 - val_loss: 2.6939\nEpoch 14/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 684ms/step - accuracy: 0.1406 - loss: 2.6959 - val_accuracy: 0.2167 - val_loss: 2.6755\nEpoch 15/15\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 677ms/step - accuracy: 0.1470 - loss: 2.7299 - val_accuracy: 0.2000 - val_loss: 2.6452\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 487ms/step - accuracy: 0.2465 - loss: 2.5938\n✅ Test Accuracy: 0.2000\n✅ Model saved as 'image_classifier_resnet.h5'\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define dataset path\ndataset_dir = \"/kaggle/working/image_dataset\"\n\n# Ensure directory exists\nos.makedirs(dataset_dir, exist_ok=True)\nshutil.make_archive(\"/kaggle/working/image_dataset_export\", 'zip', dataset_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:44:03.378258Z","iopub.execute_input":"2025-03-07T18:44:03.378806Z","iopub.status.idle":"2025-03-07T18:44:58.422012Z","shell.execute_reply.started":"2025-03-07T18:44:03.378726Z","shell.execute_reply":"2025-03-07T18:44:58.420043Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/image_dataset_export.zip'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define dataset path\ndataset_dir = \"/kaggle/working/test_dataset\"\n\n# Ensure directory exists\nos.makedirs(dataset_dir, exist_ok=True)\nshutil.make_archive(\"/kaggle/working//kaggle/working/test_dataset\", 'zip', dataset_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:45:31.438487Z","iopub.execute_input":"2025-03-07T18:45:31.439331Z","iopub.status.idle":"2025-03-07T18:45:34.249128Z","shell.execute_reply.started":"2025-03-07T18:45:31.439293Z","shell.execute_reply":"2025-03-07T18:45:34.247412Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/kaggle/working/test_dataset.zip'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"import shutil\n\nfolder_path = \"/kaggle/working/kaggle\"  # Folder to delete\n\ntry:\n    shutil.rmtree(folder_path)\n    print(f\"Deleted folder: {folder_path}\")\nexcept FileNotFoundError:\n    print(\"Folder not found, skipping deletion.\")\nexcept Exception as e:\n    print(f\"Error deleting folder: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T19:00:19.970017Z","iopub.execute_input":"2025-03-07T19:00:19.970361Z","iopub.status.idle":"2025-03-07T19:00:19.997376Z","shell.execute_reply.started":"2025-03-07T19:00:19.970334Z","shell.execute_reply":"2025-03-07T19:00:19.996038Z"}},"outputs":[{"name":"stdout","text":"Deleted folder: /kaggle/working/kaggle\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# Define paths\noriginal_dataset_dir = \"/kaggle/working/image_dataset\"  # Update this if needed\nfiltered_dataset_dir = \"/kaggle/working/filtered_image_dataset\"\nos.makedirs(filtered_dataset_dir, exist_ok=True)\n\n# Get category folders\ncategories = [d for d in os.listdir(original_dataset_dir) if os.path.isdir(os.path.join(original_dataset_dir, d))]\n\nfor category in categories:\n    category_path = os.path.join(original_dataset_dir, category)\n    new_category_path = os.path.join(filtered_dataset_dir, category)\n    os.makedirs(new_category_path, exist_ok=True)\n\n    # Select 10 random images\n    images = [f for f in os.listdir(category_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n    selected_images = random.sample(images, min(10, len(images)))  # Ensure at most 10 images\n\n    # Copy selected images to new folder\n    for image in selected_images:\n        shutil.copy(os.path.join(category_path, image), os.path.join(new_category_path, image))\n\n# Zip the dataset\nshutil.make_archive(\"/kaggle/working/filtered_image_dataset_export\", 'zip', filtered_dataset_dir)\n\nprint(\"✅ Dataset filtered & zipped successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T19:03:22.067202Z","iopub.execute_input":"2025-03-07T19:03:22.067568Z","iopub.status.idle":"2025-03-07T19:03:39.669298Z","shell.execute_reply.started":"2025-03-07T19:03:22.067539Z","shell.execute_reply":"2025-03-07T19:03:39.667836Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset filtered & zipped successfully!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import shutil\n\nfolder_path = \"/kaggle/working/test_dataset\"  # Folder to delete\n\ntry:\n    shutil.rmtree(folder_path)\n    print(f\"Deleted folder: {folder_path}\")\nexcept FileNotFoundError:\n    print(\"Folder not found, skipping deletion.\")\nexcept Exception as e:\n    print(f\"Error deleting folder: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T19:05:07.349590Z","iopub.execute_input":"2025-03-07T19:05:07.349985Z","iopub.status.idle":"2025-03-07T19:05:07.385548Z","shell.execute_reply.started":"2025-03-07T19:05:07.349953Z","shell.execute_reply":"2025-03-07T19:05:07.383928Z"}},"outputs":[{"name":"stdout","text":"Deleted folder: /kaggle/working/test_dataset\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}